{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ecolif_selenium.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyM7pFCceuMz7Ju3lpvkTlWu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/syoone/ecolif_selenium/blob/main/ecolif_selenium.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3LepWP54AF-K"
      },
      "source": [
        "pip install selenium"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cOuwFmDdoKKO"
      },
      "source": [
        "from selenium import webdriver\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "import time\n",
        "from selenium.webdriver.common.keys import Keys\n",
        "import random\n",
        "from bs4 import BeautifulSoup\n",
        "import json\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GiI8EMHe_7Vs"
      },
      "source": [
        "chrome_options = Options()\n",
        "#chrome_options.add_argument(\"--headless\")\n",
        "\n",
        "rootPath = '..'\n",
        "driver = webdriver.Chrome(\n",
        "    executable_path='{}/chrome/chromedriver74'.format(rootPath),\n",
        "    options=chrome_options\n",
        ")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CBPJK33DCiYi"
      },
      "source": [
        "#def crawl(url):\n",
        "#    headers = {\n",
        "#        \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_6) AppleWebKit/537.36 (KHTML, like Gecko)\"\n",
        "#    }\n",
        "#    data = requests.get(url, headers)\n",
        "#    print(data,url)\n",
        "#    return data.content\n",
        "\n",
        "def getProductInfo(tr):\n",
        "    tds = tr.find_all('td')\n",
        "    return {'no':tds[0].text, 'name':tds[1].text, 'catagory':tds[2].text, \n",
        "            'vendor':tds[3].text, 'confirmNo':tds[4].text, 'lincenceNo':tds[5].text}\n",
        "\n",
        "def parser(pageString):\n",
        "    bsObj = BeautifulSoup(pageString, 'html.parser')\n",
        "    table = bsObj.find('table',{'class':'table table-striped2'})\n",
        "    tbody = table.find('tbody')\n",
        "\n",
        "    trs = tbody.find_all('tr')\n",
        "\n",
        "    productInfos =[]\n",
        "    for tr in trs:\n",
        "        productInfo = getProductInfo(tr)\n",
        "        productInfos.append(productInfo)\n",
        "\n",
        "    return productInfos\n",
        "\n",
        "url = 'http://ecolife.me.go.kr/ecolife/sntryAid/index?page=6'\n",
        "driver.get(url)\n",
        "\n",
        "result = []\n",
        "def getInfos(pageNo):\n",
        "    url = 'http://ecolife.me.go.kr/ecolife/sntryAid/index?page={}'.format(pageNo)\n",
        "    driver.get(url)\n",
        "    pageSource = driver.page_source\n",
        "    infos = parser(pageSource)\n",
        "    return infos\n",
        "\n",
        "def save(df, filename):\n",
        "    writer = pd.ExcelWriter(filename)\n",
        "    df.to_excel(writer, 'Sheet1')\n",
        "    writer.save()\n",
        "\n",
        "for pageNo in range(1, 164 + 1):\n",
        "    result = result + getInfos(pageNo)\n",
        "\n",
        "df = pd.DataFrame(data=result)\n",
        "print(df.count())\n",
        "save(df, './ecolife.xlsx')\n",
        "\n",
        "print(infos[0])\n",
        "\n",
        "file = open('./result.json', 'w+')\n",
        "file.write(json.dumps(result))\n",
        "\n",
        "productInfos = []\n",
        "\n",
        "def crawlPage(pageNo):\n",
        "    url = 'http://ecolife.me.go.kr/ecolife/sntryAid/index?page={}'.format(pageNo)\n",
        "    pageString = crawl(url)\n",
        "    products = parser(pageString)\n",
        "    return products\n",
        "\n",
        "\n",
        "#--------------------- Main ----------------------------------------------------\n",
        "result = []\n",
        "\n",
        "for pageNo in range(1,10+1):  # pageNo = 171\n",
        "    result += crawlPage(pageNo)\n",
        "    df = pd.DataFrame(data=result)\n",
        "df = pd.DataFrame(data=result)\n",
        "save(df, './ecolife.xlsx')\n",
        "\n",
        "print(df.count())\n",
        "print(len(result))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}